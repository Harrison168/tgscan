#!/usr/bin/env python
# -*- encoding: utf-8 -*-
import os
import re
import fasttext
import hanlp
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import defaultdict


from spider.src.utils import natureWeight
from spider.src.utils.common import root_dir


class HanlpUtil:



    def __init__(self):
        # åŠ è½½åˆ†è¯å’Œè¯æ€§æ ‡æ³¨æ¨¡å‹
        self.segmenter = hanlp.load(hanlp.pretrained.tok.UD_TOK_MMINILMV2L12)  # åˆ†è¯æ¨¡å‹
        self.segmenter_zh = hanlp.load(hanlp.pretrained.tok.CTB9_TOK_ELECTRA_BASE)  # åˆ†è¯æ¨¡å‹
        self.pos_tagger = hanlp.load(hanlp.pretrained.pos.PTB_POS_RNN_FASTTEXT_EN)  # è¯æ€§æ ‡æ³¨æ¨¡å‹
        self.pos_tagger_zh = hanlp.load(hanlp.pretrained.pos.PKU_POS_ELECTRA_SMALL)  # è¯æ€§æ ‡æ³¨æ¨¡å‹


    # åˆ†è¯å¹¶è®¡ç®—æƒé‡
    def extract_keywords_with_pos(self, text, language_code="en", top_k=10):
        # åˆ†è¯å’Œè¯æ€§æ ‡æ³¨ç»“æœ
        if language_code == "zh":
            words = self.segmenter_zh(text)  # åˆ†è¯ç»“æœ
            tokens_with_pos = self.pos_tagger_zh(words)  # è¯æ€§æ ‡æ³¨ç»“æœ
            nature_weight = natureWeight.NATURE_WEIGHT_PKU
        else:
            words = self.segmenter(text)  # åˆ†è¯ç»“æœ
            tokens_with_pos = self.pos_tagger(words)  # è¯æ€§æ ‡æ³¨ç»“æœ
            nature_weight = natureWeight.NATURE_WEIGHT_PTB

        # words = self.segmenter_zh(text)  # åˆ†è¯ç»“æœ
        # tokens_with_pos = self.pos_tagger_zh(words)  # è¯æ€§æ ‡æ³¨ç»“æœ
        # nature_weight = natureWeight.NATURE_WEIGHT_PKU

        # print("åˆ†è¯ç»“æœ:", words)
        # print("è¯æ€§æ ‡æ³¨ç»“æœ:", tokens_with_pos)

        # å»é™¤æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼
        words_filtered = [word for word in words if re.match(r'\w+', word)]  # åªä¿ç•™å­—æ¯ã€æ•°å­—å’Œæ±‰å­—
        tokens_with_pos_filtered = [nature for word, nature in zip(words, tokens_with_pos) if re.match(r'\w+', word)]

        # å»é™¤é‡å¤è¯
        seen_words = set()
        filtered_words = []
        filtered_tokens_with_pos = []
        for word, nature in zip(words_filtered, tokens_with_pos_filtered):
            if word not in seen_words:
                seen_words.add(word)
                filtered_words.append(word)
                filtered_tokens_with_pos.append(nature)

        # è®¡ç®—è¯æƒé‡
        word_weights = []
        for word, nature in zip(filtered_words, filtered_tokens_with_pos):
            weight = nature_weight.get(nature, 0.1)  # é»˜è®¤æƒé‡ä¸º 0.1
            word_weights.append((word, weight))

        # æ ¹æ®æƒé‡æ’åºå¹¶æå–å…³é”®è¯
        sorted_keywords = sorted(word_weights, key=lambda x: x[1], reverse=True)
        keywords = [kw for kw, _ in sorted_keywords[:top_k]]

        return keywords, list(zip(filtered_words, filtered_tokens_with_pos))

    # def dataDir():
    #     data_dir = root_dir / f"data"
    #     if not os.path.exists(data_dir):
    #         os.mkdir(data_dir)
    #     return data_dir


if __name__ == '__main__':
    hanlpUtil = HanlpUtil()

    text = f"Telegram ä¸­æ–‡ç¤¾ç¾¤, Telegram ä¸­æ–‡/æ±‰åŒ–/çŸ¥è¯†/æ•™ç¨‹, ç§‘æŠ€, æœºåœº, ç§‘å­¦ä¸Šç½‘...... è‡ªç”±æ˜¯æœ‰è§„åˆ™çš„ï¼Œæ²¡è§„åˆ™çš„è‡ªç”±æ˜¯æ··ä¹±ã€‚ ç¦è¨€æ˜¯æœºå™¨äººå¹²çš„ï¼Œé€€ç¾¤é‡æ–°åŠ å…¥å³å¯ã€‚ ç¾¤è§„: * ç¦æ­¢ä¼ æ’­è°£è¨€/ç›—ç‰ˆ * ç¦æ­¢æ’•é€¼/è°©éª‚/äººèº«æ”»å‡»/è¡€è…¥/æš´åŠ› * ç¦æ­¢è®¨è®ºå…æµ/é»„èµŒæ¯’/å®—æ•™/æ”¿æ²»/é”®æ”¿ * ç¦æ­¢å¹¿å‘Š/æ¨å¹¿/é»‘äº§/ç°äº§/æš—ç½‘/åˆ·å±/è‰²æƒ…/å¼€è½¦/NSFW * ä»…é™èŠä¸­æ–‡è¯é¢˜ ğŸ‘¥Telegram äºŒåä¸‡äºº @tgcnx ğŸ‘¥Telegram ä¸­æ–‡äºŒç¾¤ @tgzhcn ğŸ“¢Telegram ä¸­æ–‡é¢‘é“ @tgcnz"
    text_en = f"Coinlist ( é¦™æ¸¯-ç¡¬å¸æ¸…å•ï¼‰ - ğ™¾ğšƒğ™² ğ™¼ğ™°ğšğ™ºğ™´ğšƒ , ğš„ğ™¿ğ™·ğ™¾ğ™»ğ™³ , ğ™±ğ™¸ğ™½ğ™°ğ™½ğ™²ğ™´ , ğ™·ğš„ğ™¾ğ™±ğ™¸ , ğ™°ğ™½ğ™³ ğ™¾ğšƒğ™·ğ™´ğšğš‚ğŸ’²ğŸ”¥"
    text_en2 = f"SHARE FREE AIRDROP - ARBITRUM BSC LOW TAX - SHILL HIDDEN GEM"
    # tokens = tokenizer(text)
    # print(tokens)

    # æ‰‹åŠ¨å®ç° TF-IDF æå–å…³é”®è¯ï¼ˆéœ€è¦è‡ªå·±æ„å»ºè¯é¢‘å’Œé€†æ–‡æ¡£é¢‘ç‡çŸ©é˜µï¼‰
    # vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)
    # tfidf_matrix = vectorizer.fit_transform([tokens])
    # feature_names = vectorizer.get_feature_names_out()
    # scores = tfidf_matrix.toarray()[0]
    # # è·å–å¾—åˆ†æœ€é«˜çš„è¯
    # top_keywords = sorted(zip(feature_names, scores), key=lambda x: x[1], reverse=True)[:10]
    # print("Top Keywords:", top_keywords)

    # æå–å…³é”®è¯
    keywords, pos_tags = hanlpUtil.extract_keywords_with_pos(text_en, language_code="en", top_k=10)
    print("å…³é”®è¯ï¼š", keywords)
    print("è¯æ€§æ ‡æ³¨ï¼š", pos_tags)
    keywords, pos_tags = hanlpUtil.extract_keywords_with_pos(text_en2, language_code="en", top_k=10)
    print("å…³é”®è¯ï¼š", keywords)
    print("è¯æ€§æ ‡æ³¨ï¼š", pos_tags)


    # å®šä¹‰å¤šåŠŸèƒ½åˆ†è¯å™¨
    # pipeline = hanlp.pipeline() \
    #     .append(hanlp.utils.rules.split_sentence, output_key='sentences') \
    #     .append(segmenter, output_key='tokens') \
    #     .append(pos_tagger, output_key='part_of_speech_tags')
    # output = pipeline(text_en)
    # print(output)


    # model = fasttext.load_model('lid.176.bin')
    # prediction = model.predict(text)
    # language = prediction[0][0].split('__')[-1]
    # print(f"The detected language is: {language}")














